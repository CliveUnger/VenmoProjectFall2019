{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpful links for processing graphs on pyspark:\n",
    "\n",
    "https://docs.databricks.com/spark/latest/graph-analysis/graphframes/user-guide-python.html\n",
    "\n",
    "https://graphframes.github.io/graphframes/docs/_site/user-guide.html\n",
    "\n",
    "https://pysparktutorial.blogspot.com/2017/10/graphframes-pyspark.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.sql.functions import col, size, lit\n",
    "import pyspark.sql.functions as F\n",
    "from graphframes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version:  2.4.4\n",
      "defaultParallelism:  44\n",
      "Spark WebURLL  http://c251-117.wrangler.tacc.utexas.edu:4040\n"
     ]
    }
   ],
   "source": [
    "# Start spark in local mode using 54gb of memory\n",
    "# local mode only runs on a single node, but it will utilize all cores (We have 48!)\n",
    "conf = SparkConf().setAppName(\"test\") \\\n",
    "    .setMaster(\"local[44]\") \\\n",
    "    .set('spark.driver.memory','54g') \\\n",
    "    .set('spark.jars.packages', 'graphframes:graphframes:0.7.0-spark2.4-s_2.11')\n",
    "#.setMaster(\"yarn\") # this is used when we run on hadoop, ignore for now\n",
    "\n",
    "sc = SparkContext(conf = conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "print(\"Spark Version: \", sc.version)\n",
    "print(\"defaultParallelism: \", sc.defaultParallelism)\n",
    "print(\"Spark WebURLL \", sc.uiWebUrl) # you can view running jobs here, but I am only able to connect to it via VNC rn, maybe SSH tunneling will fix this? idk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.jars.packages', 'graphframes:graphframes:0.7.0-spark2.4-s_2.11'),\n",
       " ('spark.app.name', 'test'),\n",
       " ('spark.driver.memory', '54g'),\n",
       " ('spark.files',\n",
       "  'file:///home/06271/cju256/.ivy2/jars/graphframes_graphframes-0.7.0-spark2.4-s_2.11.jar,file:///home/06271/cju256/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar'),\n",
       " ('spark.app.id', 'local-1572750743715'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.local.dir', '/data/06271/cju256/temp'),\n",
       " ('spark.driver.host', 'c251-117.wrangler.tacc.utexas.edu'),\n",
       " ('spark.master', 'local[44]'),\n",
       " ('spark.submit.pyFiles',\n",
       "  '/home/06271/cju256/.ivy2/jars/graphframes_graphframes-0.7.0-spark2.4-s_2.11.jar,/home/06271/cju256/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar'),\n",
       " ('spark.driver.port', '45129'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///home/06271/cju256/.ivy2/jars/graphframes_graphframes-0.7.0-spark2.4-s_2.11.jar,file:///home/06271/cju256/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.jars',\n",
       "  'file:///home/06271/cju256/.ivy2/jars/graphframes_graphframes-0.7.0-spark2.4-s_2.11.jar,file:///home/06271/cju256/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.getAll() # See all the current Spark configuration settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_path = '/data/06271/cju256/nodes.json'\n",
    "edges_path = '/data/06271/cju256/edges.json'\n",
    "\n",
    "nodes = sqlContext.read.json(nodes_path)\n",
    "edges = sqlContext.read.json(edges_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- about: string (nullable = true)\n",
      " |-- cancelled: boolean (nullable = true)\n",
      " |-- date_created: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- external_id: string (nullable = true)\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- friends: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- is_business: boolean (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- num_friends: long (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- picture: string (nullable = true)\n",
      " |-- username: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nodes.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- src: string (nullable = true)\n",
      " |-- dst: string (nullable = true)\n",
      " |-- weight: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edges.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType, LongType\n",
    "\n",
    "just_nodes = nodes.withColumn(\"id_string\", col('id').cast(LongType())).drop('id').withColumnRenamed(\"id_string\",'id')\n",
    "just_edges = edges \\\n",
    "                .withColumn(\"src_string\", col('src').cast(LongType())).drop('src').withColumnRenamed(\"src_string\",'src') \\\n",
    "                .withColumn(\"dst_string\", col('dst').cast(LongType())).drop('dst').withColumnRenamed(\"dst_string\",'dst')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "just_edges = just_edges.fillna('0', subset=['dst'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphFrame(v:[id: bigint, about: string ... 13 more fields], e:[src: bigint, dst: bigint ... 1 more field])\n"
     ]
    }
   ],
   "source": [
    "g = GraphFrame(just_nodes, just_edges)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[about: string, cancelled: boolean, date_created: string, email: string, external_id: string, firstname: string, friends: string, is_business: boolean, lastname: string, name: string, num_friends: bigint, phone: string, picture: string, username: string, id: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[weight: string, src: bigint, dst: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(g.vertices)\n",
    "display(g.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "| max(id)|\n",
      "+--------+\n",
      "|41493705|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g.vertices.agg({\"id\": \"max\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes:  23133264\n",
      "Edges:  342281006\n"
     ]
    }
   ],
   "source": [
    "print(\"Nodes: \", g.vertices.count())\n",
    "print(\"Edges: \", g.edges.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes:  23133264\n",
      "Edges:  132514256\n"
     ]
    }
   ],
   "source": [
    "print(\"Nodes: \", g.vertices.count())\n",
    "print(\"Edges: \", g.edges.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for pairs of vertices with edges in both directions between them.\n",
    "motifs = g.find(\"(a)-[e]->(b); (b)-[e2]->(a)\")\n",
    "# More complex queries can be expressed by applying filters.\n",
    "#motifs.filter(\"a.id != c.id\").show()\n",
    "motifs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#motifs.filter(\"a.id != c.id\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
